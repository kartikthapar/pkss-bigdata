package edu.jhu.bdslss.pkss;

import java.io.IOException;
import java.util.List;
import org.apache.hadoop.fs.BlockLocation;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.InputSplit;
import edu.jhu.bdslss.VectorizedObject;

public class InputFormat extends org.apache.hadoop.mapreduce.lib.input.FileInputFormat<LongWritable, VectorizedObject>
{
    @Override
    public org.apache.hadoop.mapreduce.RecordReader<LongWritable, VectorizedObject> createRecordReader(
            org.apache.hadoop.mapreduce.InputSplit split,
            org.apache.hadoop.mapreduce.TaskAttemptContext context)
    {
        // We'll just assume that the split is a fileSplit
        return new edu.jhu.bdslss.pkss.RecordReader();
    }

    /*@Override
    public List<InputSplit> getSplits(
            org.apache.hadoop.mapreduce.JobContext job)
        throws IOException
    {
        //Stopwatch sw = new Stopwatch().start();
        long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
        long maxSize = InputFormat.getMaxSplitSize(job);

        // generate splits
        List<InputSplit> splits = new java.util.ArrayList<InputSplit>();
        List<FileStatus> files = listStatus(job);
        for (FileStatus file: files) {
            Path path = file.getPath();
            long length = file.getLen();
            if (length != 0) {
                BlockLocation[] blkLocations;
                FileSystem fs = path.getFileSystem(job.getConfiguration());
                blkLocations = fs.getFileBlockLocations(file, 0, length);

                if (isSplitable(job, path)) {
                    long blockSize = file.getBlockSize();
                    long splitSize = computeSplitSize(blockSize, minSize, maxSize);

                    long bytesRemaining = length;
                    while (((double) bytesRemaining)/splitSize > SPLIT_SLOP) {
                      int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
                      splits.add(makeSplit(path, length-bytesRemaining, splitSize,
                                  blkLocations[blkIndex].getHosts(),
                                  blkLocations[blkIndex].getCachedHosts()));
                      bytesRemaining -= splitSize;
                    }

                    if (bytesRemaining != 0) {
                      int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
                      splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,
                                 blkLocations[blkIndex].getHosts(),
                                 blkLocations[blkIndex].getCachedHosts()));
                    }
                } else { // not splitable
                  splits.add(makeSplit(path, 0, length, blkLocations[0].getHosts(),
                              blkLocations[0].getCachedHosts()));
                }
            }
            else
            {
              //Create empty hosts array for zero length files
              splits.add(makeSplit(path, 0, length, new String[0]));
            }
        }

        // Save the number of input files for metrics/loadgen
        job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());
        //sw.stop();
        if (LOG.isDebugEnabled()) {
          LOG.debug("Total # of splits generated by getSplits: " + splits.size()
              + ", TimeTaken: " + sw.elapsedMillis());
        }
        return splits;
    }*/
}
